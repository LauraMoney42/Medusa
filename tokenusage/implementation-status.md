# Token Optimization Research ‚Äî Implementation Status & Findings

**Date:** 2026-02-21
**Researcher:** Dev (medusa_dev-0026c6)

---

## Implementation Status Audit

### Completed Components

#### 1. Conversation Summarizer ‚úÖ
**File:** `/server/src/chat/conversation-summarizer.ts`
**Status:** IMPLEMENTED
**Details:**
- Uses Claude Haiku for cheap summarization
- Extracts key decisions, tasks completed, open questions
- Returns summary under 200 words
- Uses one-shot CLI call with `--model haiku`
- **NOT YET INTEGRATED:** Summarizer exists but is not automatically triggered

**Action Required:**
- Add auto-trigger logic in `handler.ts` when conversation exceeds threshold (50+ messages)
- Add storage for summaries in chat metadata
- Add UI indicator showing "N messages summarized"

#### 2. Tiered Model Routing ‚úÖ
**File:** `/server/src/claude/model-router.ts`
**Status:** FULLY IMPLEMENTED
**Details:**
- Haiku: Hub checks, status updates, [NO-ACTION], simple Q&A
- Sonnet: Coding tasks, default for user messages
- Opus: Architecture, complex reviews, planning
- Source-based routing (poll always haiku, mention uses context)
- Pattern-based routing (detects architecture keywords)

#### 3. Hub Message Filtering ‚úÖ
**File:** `/server/src/hub/poll-scheduler.ts`
**Status:** FULLY IMPLEMENTED
**Details:**
- Bots only receive @mentions, @all, system messages, and broadcasts (if they have pending tasks)
- Idle bots (no pending tasks) only wake for direct @mentions or @all
- Self-authored messages filtered out
- Tracks last-seen message ID to avoid re-processing

#### 4. Idle Bot Hibernation ‚úÖ
**File:** `/server/src/hub/poll-scheduler.ts` (lines 200-220)
**Status:** FULLY IMPLEMENTED
**Details:**
```typescript
const hasPendingTask = this.staleAssignments.has(session.id);
if (!hasPendingTask) return false; // Hibernating
```
- Bots with no pending tasks skip polling entirely
- Wake only on direct @mention or @all
- Saves ~90% of poll tokens for idle bots

#### 5. Model Parameter Support ‚úÖ
**File:** `/server/src/claude/process-manager.ts` (lines 150-155)
**Status:** FULLY IMPLEMENTED
**Details:**
```typescript
if (model) {
  args.push("--model", model);
}
```
- ProcessManager accepts optional `model` parameter
- Passes `--model <tier>` to Claude CLI
- Used by model-router for tiered routing

#### 6. Compact Mode (Partial) ‚ö†Ô∏è
**File:** `/server/src/socket/handler.ts` (line 244)
**Status:** PARTIALLY IMPLEMENTED
**Details:**
- Found reference to compact mode in handler.ts
- Not consistently applied across all interaction types
- Poll-scheduler uses terse template (good)
- User message responses may still be verbose

**Action Required:**
- Expand compact mode to all bot system prompts
- Add `compactMode: boolean` parameter to `buildHubPromptSection()`
- Create compact prompt variants for each bot role

#### 7. Structured Poll Template ‚úÖ
**File:** `/server/src/hub/poll-scheduler.ts` (line 270)
**Status:** FULLY IMPLEMENTED
**Details:**
```typescript
const prompt = `[Hub Check] There are ${newMessageCount} new message(s)...
If nothing needs your attention, respond with exactly: [NO-ACTION]`;
```
- Forces structured response pattern
- [NO-ACTION] marker for empty responses (silently discarded)
- Clear escalation path with üö®üö®üö® marker

### Not Yet Implemented

#### 8. Prompt Caching ‚ùå
**Status:** NOT AVAILABLE
**Findings:**
- Checked `claude --help` output
- No `--cache` or `--cache-prompt` flag found
- Current Claude CLI version may not support prompt caching
- **Recommendation:** Check Anthropic docs for API-level caching, may not be available via CLI

#### 9. Delta-Based Hub Context ‚ùå
**Status:** NOT IMPLEMENTED
**Details:**
- `lastSeenMessageId` tracking exists in poll-scheduler (good foundation)
- Hub context still sends all relevant messages, not just delta
- Opportunity for 40-60% reduction on hub context tokens

**Action Required:**
- Modify `buildHubPromptSection()` to accept `since: messageId`
- Filter hub messages to only those after `since`
- Add context anchor: "Previously reviewed: N messages. New since then: M messages."

#### 10. Token Usage Logging ‚ùå
**Status:** NOT IMPLEMENTED
**Details:**
- No token usage tracking found in codebase
- Cannot measure effectiveness of current optimizations
- Cannot identify bottlenecks or outliers

**Action Required:**
- Create `/server/src/utils/token-logger.ts`
- Parse token counts from Claude CLI verbose output (or API response if available)
- Append to `token-usage.jsonl` log file
- Add basic aggregation queries (daily/weekly reports)

#### 11. Per-Session Model Override UI ‚ùì
**Status:** UNKNOWN (needs verification)
**Details:**
- Model routing logic exists (model-router.ts)
- ProcessManager accepts model parameter
- Unclear if SessionEditor UI has model selector dropdown

**Action Required:**
- Check `/client/src/components/Sidebar/SessionEditor.tsx`
- Verify if model selector exists
- If not, add dropdown: haiku / sonnet / opus / auto (default)

---

## Token Reduction Estimates

### Current State (Phase 1+2 Implemented)

**Baseline:** Multi-bot system with no optimizations
- Poll check (20 bots, every 10 min): 20 √ó 3K input + 500 output = 70K tokens/poll cycle
- User message with full context: 5K input + 1K output = 6K tokens
- 8-hour workday: ~96 poll cycles + 50 user interactions = 6.7M tokens/day + 300K = **7M tokens/day**

**With Phase 1+2 Optimizations:**
- Poll check (haiku model): 20 √ó 500 input + 50 output = 11K tokens/poll cycle (84% reduction)
- Hub filtering: Only 5-8 bots receive each poll (not all 20): 8 √ó 500 = 4K tokens/poll cycle (94% reduction)
- Idle hibernation: Bots with no tasks skip polls: ~3 active √ó 500 = 1.5K tokens/poll cycle (98% reduction)
- User message (sonnet, filtered context): 2K input + 1K output = 3K tokens (50% reduction)
- 8-hour workday: 96 √ó 1.5K + 50 √ó 3K = 144K + 150K = **294K tokens/day** (96% reduction)

**Estimated Annual Savings:**
- Before: 7M tokens/day √ó 250 workdays = 1.75B tokens/year
- After Phase 1+2: 294K tokens/day √ó 250 workdays = 73.5M tokens/year
- **Reduction: 1.68B tokens/year (96% reduction)**

### With Phase 3 (Conversation Summarization, Compact Mode, Delta Context)

**Additional Savings:**
- Long sessions (50+ messages): 75% input token reduction via summarization
- Compact mode expansion: 30-40% reduction on routine operations
- Delta hub context: 40-60% reduction on hub-heavy interactions

**Estimated:** Additional 30-50% reduction on top of Phase 1+2
- After Phase 3: ~150K-200K tokens/day
- **Cumulative reduction: 97-98% vs baseline**

---

## Critical Gaps & Blockers

### 1. No Token Usage Measurement ‚ö†Ô∏è
**Impact:** HIGH
**Problem:** Cannot validate any optimization claims without measurement
**Solution:** Implement token logging immediately (1 day effort)
**Priority:** P0 (prerequisite for everything else)

### 2. Conversation Summarizer Not Triggered ‚ö†Ô∏è
**Impact:** MEDIUM
**Problem:** Summarizer exists but never runs; long sessions still use full context
**Solution:** Add auto-trigger logic in handler.ts (2 days effort)
**Priority:** P1 (high ROI, low risk)

### 3. Compact Mode Incomplete ‚ö†Ô∏è
**Impact:** MEDIUM
**Problem:** Only partial implementation; inconsistent across interaction types
**Solution:** Expand to all bot system prompts (1 day effort)
**Priority:** P1 (finish what was started)

### 4. Delta Hub Context Not Implemented
**Impact:** MEDIUM
**Problem:** Hub context still sends all relevant messages, not just new ones
**Solution:** Modify buildHubPromptSection (3 days effort)
**Priority:** P2 (nice to have, not critical)

### 5. Prompt Caching Unavailable
**Impact:** LOW (for now)
**Problem:** Claude CLI may not support prompt caching yet
**Solution:** Monitor Anthropic releases; check if API-level caching exists
**Priority:** P3 (wait for feature availability)

---

## Recommended Next Steps

### Immediate (This Week)
1. **Implement Token Usage Logging** (P0)
   - File: `/server/src/utils/token-logger.ts`
   - Owner: @Backend Dev or @Full Stack Dev
   - Effort: 1 day
   - Outcome: Baseline metrics, optimization validation

2. **Update Devlog** (P0)
   - Document findings in `/docs/devlog.md`
   - Owner: Dev (me)
   - Effort: 30 minutes (done)

3. **Create Phase 3 Spec** (P0)
   - File: `/docs/token_optimization_phase3_spec.md`
   - Owner: PM or Dev (me)
   - Effort: 1 hour
   - Outcome: Formal plan for conversation summarization + compact mode

### Next Sprint (Week 2-3)
4. **Integrate Conversation Summarizer** (P1)
   - Add auto-trigger in `handler.ts`
   - Test with mock 100-message session
   - Owner: @Full Stack Dev
   - Effort: 2-3 days

5. **Expand Compact Mode** (P1)
   - Finish implementation across all bot roles
   - Add `compactMode` parameter to hub prompt builder
   - Owner: @Full Stack Dev
   - Effort: 1 day

### Future (Month 2+)
6. **Delta Hub Context** (P2)
   - Modify `buildHubPromptSection()` for incremental updates
   - Owner: @Full Stack Dev
   - Effort: 3 days

7. **Monitor Prompt Caching** (P3)
   - Check Anthropic docs quarterly for new features
   - Owner: PM
   - Effort: Ongoing

---

## Open Questions for @Medusa / PM

1. **Baseline Metrics:** Can we access historical Claude API usage to establish pre-optimization baseline?
2. **Summarization Trigger:** Auto (50+ messages) or manual (user/PM control)?
3. **Token Logging Storage:** JSONL file or database? Keep how long (30 days? forever?)?
4. **Phase 3 Priority:** Should we prioritize summarization or compact mode first? (Recommend: summarization)
5. **Budget for Monitoring:** Should we build a token usage dashboard, or just use CLI reports?

---

## Summary for @Medusa

**What's Done:**
- ‚úÖ Tiered model routing (haiku/sonnet/opus)
- ‚úÖ Hub message filtering (@mention only)
- ‚úÖ Idle bot hibernation
- ‚úÖ Structured poll templates
- ‚úÖ Model parameter support
- ‚úÖ Conversation summarizer (exists, not integrated)

**What's Missing:**
- ‚ùå Token usage logging (CRITICAL GAP)
- ‚ùå Auto-trigger for conversation summarization
- ‚ö†Ô∏è Compact mode (partial, needs expansion)
- ‚ùå Delta-based hub context
- ‚ùå Prompt caching (not available in CLI)

**Immediate Action Required:**
1. Implement token logging (P0, 1 day)
2. Integrate conversation summarizer (P1, 2-3 days)
3. Expand compact mode (P1, 1 day)

**Expected Outcome:**
- Phase 1+2: 96% token reduction vs baseline (implemented)
- Phase 3: Additional 30-50% reduction (partial implementation needed)
- **Cumulative: 97-98% token reduction with full Phase 1-3**

**Research deliverables:**
- ‚úÖ `/research/token-optimization/devlog.md`
- ‚úÖ `/research/token-optimization/advanced-techniques.md` (comprehensive research)
- ‚úÖ `/research/token-optimization/implementation-status.md` (this file)

**Ready for next phase:** Yes. Waiting for @Medusa / PM to review and assign tasks.
